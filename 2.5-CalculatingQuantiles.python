{"version":"NotebookV1","origId":1456266427964810,"name":"2.5-CalculatingQuantiles","language":"python","commands":[{"version":"CommandV1","origId":1214224320307739,"guid":"13dd13bb-34c7-4e06-ab2a-7514aa515e24","subtype":"command","commandType":"auto","position":5.5,"command":"monthsRDDs=[i for i in xrange(12)]\nrdds=dataRDD.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291771552,"submitTime":1500291729705,"finishTime":1500291772760,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"fc9ebc44-b1d2-4979-bf2f-c1686fc79986"},{"version":"CommandV1","origId":1214224320307740,"guid":"38cc1294-547a-432f-a1f8-7f58307aa788","subtype":"command","commandType":"auto","position":5.75,"command":"for i in xrange(12):\n  if i<5:\n    monthsRDDs[i]=sorted(rdds[i][1],key=lambda x:x)\n  else: \n    if i>5:\n      monthsRDDs[i]=sorted(rdds[i-1][1],key=lambda x:x)\n    ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-18-ade3f91d02e7&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">3</span>\n<span class=\"ansiyellow\">    monthsRDDs[i]=sorted(rdds[i][1],,key=lambda x:x)</span>\n<span class=\"ansigrey\">                                    ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n</div>","workflows":[],"startTime":1500291772769,"submitTime":1500291729711,"finishTime":1500291772841,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f5064a15-8f20-46a9-8ba3-25cd531a00ee"},{"version":"CommandV1","origId":1214224320307741,"guid":"8bea08e0-f8aa-42e0-a389-fe08b3acc6d1","subtype":"command","commandType":"auto","position":5.875,"command":"monthsRDDs[0]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>\n[2714,\n 2773,\n 3101,\n 3343,\n 3559,\n 3641,\n 3678,\n 3872,\n 4255,\n 4943,\n 5144,\n 5437,\n 5786,\n 5805,\n 6178,\n 6239,\n 6381,\n 6549,\n 6882,\n 7231,\n 7376,\n 7502,\n 10317,\n 10783,\n 11992,\n 12016,\n 12917,\n 14037,\n 14591,\n 15037,\n 17892]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291772844,"submitTime":1500291729717,"finishTime":1500291772913,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"65343561-0a46-4603-bf2d-4856bea7c3d9"},{"version":"CommandV1","origId":1214224320307742,"guid":"d068df3d-37cd-446e-90bf-556b90155738","subtype":"command","commandType":"auto","position":11.0,"command":"may=[[x] for x in monthsRDDs[4]]\ndf=sqlContext.createDataFrame(data=may,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[5067],[5561],[5867],[5904],[5930],[5934],[5978],[6130],[6169],[6340],[6341],[6376],[6377],[6379],[6592],[7392],[7970],[8934],[9505],[9802]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291774743,"submitTime":1500291729749,"finishTime":1500291774942,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"dc3781e7-3365-4ab0-af1c-13dd6603d160"},{"version":"CommandV1","origId":1214224320307743,"guid":"84065dfb-3a5a-482a-b16c-6e1fb7dfcaff","subtype":"command","commandType":"auto","position":12.0,"command":"july=[[x] for x in monthsRDDs[6]]\ndf=sqlContext.createDataFrame(data=july,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[3597],[7680],[8125],[8791],[10433],[10463],[10908]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291774945,"submitTime":1500291729754,"finishTime":1500291775176,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"avg","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"c9fef642-6b26-484e-8d4e-b23296ac0552"},{"version":"CommandV1","origId":1214224320307744,"guid":"4fd6aca5-1493-441b-9090-1c1f8d973ba7","subtype":"command","commandType":"auto","position":13.0,"command":"august=[[x] for x in monthsRDDs[7]]\ndf=sqlContext.createDataFrame(data=august,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[5899],[7316],[7548],[7675],[7754],[8267],[8407],[8435],[8630],[8839],[8982],[9133],[9277],[9632],[9744],[9806],[9916],[10073],[10672],[10821],[11546],[12092]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291775180,"submitTime":1500291729760,"finishTime":1500291775458,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"748407c7-c810-4b86-b4e0-6ae1694b407f"},{"version":"CommandV1","origId":1214224320307745,"guid":"d9d6f327-8dac-4936-94a6-c483fd6fc174","subtype":"command","commandType":"auto","position":14.0,"command":"september=[[x] for x in monthsRDDs[8]]\ndf=sqlContext.createDataFrame(data=september,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[7962],[9625],[10806],[11553],[11670],[11782],[11836],[11879],[12492],[12541],[12723],[12734],[12974],[13204],[13264],[13548],[13810],[13917],[14765],[14918],[15078],[15231],[15383],[16425],[17590],[18036],[18361],[18795],[21101],[23946]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291775462,"submitTime":1500291729766,"finishTime":1500291775661,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"5df2df82-0292-445f-87bb-f99e0c819416"},{"version":"CommandV1","origId":1214224320307746,"guid":"da2f30ed-1c89-4cae-9ef3-01e960e16f98","subtype":"command","commandType":"auto","position":15.0,"command":"october=[[x] for x in monthsRDDs[9]]\ndf=sqlContext.createDataFrame(data=october,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[6162],[6183],[6735],[7004],[7373],[7615],[8090],[8188],[8676],[9263],[9317],[9679],[9754],[10282],[10567],[10955],[11061],[11151],[11187],[11869],[12128],[12173],[12185],[14906],[17620],[18440],[19219],[19307],[20226],[20647],[21866]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291775664,"submitTime":1500291729771,"finishTime":1500291775896,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"19478855-2f44-4ff1-a207-49b23a2de987"},{"version":"CommandV1","origId":1214224320307747,"guid":"ef8b5ffa-6b8c-434e-956b-5ec9a8bcae4a","subtype":"command","commandType":"auto","position":16.0,"command":"november=[[x] for x in monthsRDDs[10]]\ndf=sqlContext.createDataFrame(data=november,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[2708],[4681],[4773],[4943],[5145],[5289],[5567],[5770],[5927],[6141],[6433],[6923],[7091],[7696],[7790],[7852],[7944],[8125],[8351],[8395],[8553],[8555],[9037],[11253],[12574],[18595],[18912],[19337],[19600],[25578]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291775899,"submitTime":1500291729777,"finishTime":1500291776180,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"413cadca-a87e-42cf-9c12-ea594b395436"},{"version":"CommandV1","origId":1214224320307748,"guid":"7ce5363b-7ee6-4c5b-8c91-313d0f07415a","subtype":"command","commandType":"auto","position":17.0,"command":"december=[[x] for x in monthsRDDs[11]]\ndf=sqlContext.createDataFrame(data=december,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[4580],[8474],[9033],[9048],[9286],[9561],[9704],[9711],[9871],[9923],[9940],[10690],[11128],[11979],[12269],[12839],[13019],[13656],[14622],[14840],[14933],[15866],[16556],[17376],[18475],[21164],[22743],[25117],[25425],[26284],[30028]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291776186,"submitTime":1500291729782,"finishTime":1500291776468,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}],"quantilePlot":[]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"abae1654-9425-4af3-8e64-bf063f42c9bb"},{"version":"CommandV1","origId":1456266427964812,"guid":"9905a21e-75e6-4ad4-a8f4-b223622e352b","subtype":"command","commandType":"auto","position":1.0,"command":"data=sqlContext.sql(\"Select day,first(month) as month, sum(fw) from joineddata where hour>=12 and hour<17 group by day  \")\ndata.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+-----+-------+\n|day|month|sum(fw)|\n+---+-----+-------+\n| 26|    1|   2714|\n| 29|    1|   6882|\n| 65|    3|   4359|\n|270|    9|  11670|\n|293|   10|   9317|\n|278|   10|  12128|\n|243|    8|  12092|\n|296|   10|  19307|\n| 54|    2|   4160|\n| 19|    1|   5786|\n|287|   10|   8090|\n|348|   12|   9704|\n|277|   10|  11869|\n|113|    4|  20331|\n|112|    4|  20158|\n|299|   10|   7615|\n|237|    8|   7675|\n|241|    8|   7316|\n|347|   12|   8474|\n|330|   11|   5289|\n+---+-----+-------+\nonly showing top 20 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">AnalysisException</span>: u&quot;cannot resolve &apos;&#96;doy&#96;&apos; given input columns: [day, fw, bw, hour, month, dow, description, temperature]; line 1 pos 47;\\n&apos;Aggregate [&apos;doy], [month#489L, sum(fw#494L) AS sum(fw)#512L]\\n+- SubqueryAlias joineddata\\n   +- Relation[day#488L,month#489L,dow#490L,hour#491L,description#492L,temperature#493,fw#494L,bw#495L] parquet\\n&quot;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-3-4b6201f4824c&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>data<span class=\"ansiyellow\">=</span>sqlContext<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Select month, sum(fw) from joineddata group by doy  &quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> data<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/context.pyc</span> in <span class=\"ansicyan\">sql</span><span class=\"ansiblue\">(self, sqlQuery)</span>\n<span class=\"ansigreen\">    382</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    383</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 384</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>sparkSession<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sqlQuery<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    385</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    386</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.pyc</span> in <span class=\"ansicyan\">sql</span><span class=\"ansiblue\">(self, sqlQuery)</span>\n<span class=\"ansigreen\">    543</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    544</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 545</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sqlQuery<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_wrapped<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    546</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    547</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: u&quot;cannot resolve &apos;&#96;doy&#96;&apos; given input columns: [day, fw, bw, hour, month, dow, description, temperature]; line 1 pos 47;\\n&apos;Aggregate [&apos;doy], [month#489L, sum(fw#494L) AS sum(fw)#512L]\\n+- SubqueryAlias joineddata\\n   +- Relation[day#488L,month#489L,dow#490L,hour#491L,description#492L,temperature#493,fw#494L,bw#495L] parquet\\n&quot;</div>","workflows":[],"startTime":1500291729811,"submitTime":1500291729646,"finishTime":1500291739560,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"dd88e540-1897-427c-ab94-dc74a773680f"},{"version":"CommandV1","origId":1456266427964846,"guid":"249ef8f5-eb0d-4a43-a86b-c635a7678b6d","subtype":"command","commandType":"auto","position":2.0,"command":"dataRDD=data.rdd.map(lambda x:(x[1],[int(x[2])]))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291739564,"submitTime":1500291729647,"finishTime":1500291739685,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"873ae079-2257-4696-98c6-7a5f2a057d9a"},{"version":"CommandV1","origId":1456266427964847,"guid":"44ca69a2-5a31-45e6-8647-b6760afd7af8","subtype":"command","commandType":"auto","position":3.0,"command":"dataRDD=dataRDD.reduceByKey(lambda x,y:x+y)\ndataRDD.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>\n[(1,\n  [2714,\n   6882,\n   5786,\n   6549,\n   15037,\n   3101,\n   2773,\n   14037,\n   5144,\n   3559,\n   4255,\n   7376,\n   11992,\n   17892,\n   5437,\n   12016,\n   4943,\n   12917,\n   6239,\n   14591,\n   10317,\n   6178,\n   7231,\n   10783,\n   6381,\n   7502,\n   3872,\n   3343,\n   3641,\n   5805,\n   3678]),\n (2,\n  [4160,\n   3727,\n   7447,\n   10766,\n   5716,\n   3277,\n   3202,\n   5707,\n   4251,\n   9036,\n   4520,\n   3180,\n   1774,\n   5045,\n   4201,\n   3310,\n   4613,\n   8529,\n   5064,\n   2568,\n   3489,\n   4289,\n   6875,\n   5129,\n   3267,\n   4916,\n   4426,\n   4973]),\n (3,\n  [4359,\n   9833,\n   10529,\n   8977,\n   5032,\n   4818,\n   4550,\n   4813,\n   4624,\n   5306,\n   5316,\n   6778,\n   5049,\n   5498,\n   4431,\n   4686,\n   4373,\n   4540,\n   4836,\n   5758,\n   5299,\n   10010,\n   5619,\n   4073,\n   3746,\n   4878,\n   5934,\n   9972,\n   4759,\n   5116,\n   7383]),\n (4,\n  [20331,\n   20158,\n   5525,\n   11401,\n   22741,\n   10722,\n   2550,\n   5678,\n   16601,\n   9886,\n   15903,\n   12653,\n   9998,\n   11031,\n   26668,\n   11905,\n   13362,\n   16062,\n   7073,\n   2527,\n   5837,\n   7475,\n   11667,\n   9346,\n   4964,\n   7417,\n   5190,\n   10076]),\n (5,\n  [6341,\n   8934,\n   5867,\n   5934,\n   5561,\n   6377,\n   6130,\n   6169,\n   6340,\n   6379,\n   6376,\n   7970,\n   5067,\n   9505,\n   6592,\n   9802,\n   5978,\n   5930,\n   5904,\n   7392]),\n (7, [10908, 8791, 7680, 3597, 10463, 10433, 8125]),\n (8,\n  [12092,\n   7675,\n   7316,\n   10672,\n   9744,\n   9632,\n   9916,\n   8839,\n   8407,\n   10821,\n   5899,\n   7754,\n   7548,\n   9806,\n   9277,\n   11546,\n   8435,\n   8630,\n   8267,\n   9133,\n   10073,\n   8982]),\n (9,\n  [11670,\n   11836,\n   18361,\n   13548,\n   21101,\n   17590,\n   9625,\n   10806,\n   11782,\n   14918,\n   13917,\n   16425,\n   12974,\n   15078,\n   15383,\n   15231,\n   18036,\n   13264,\n   11879,\n   18795,\n   7962,\n   12541,\n   11553,\n   13204,\n   12723,\n   13810,\n   14765,\n   12734,\n   23946,\n   12492]),\n (10,\n  [9317,\n   12128,\n   19307,\n   8090,\n   11869,\n   7615,\n   19219,\n   12185,\n   18440,\n   21866,\n   9754,\n   10955,\n   20226,\n   6735,\n   10282,\n   11151,\n   7373,\n   11061,\n   20647,\n   17620,\n   10567,\n   8188,\n   14906,\n   11187,\n   8676,\n   9679,\n   6183,\n   9263,\n   6162,\n   12173,\n   7004]),\n (11,\n  [5289,\n   12574,\n   2708,\n   7696,\n   9037,\n   8555,\n   7091,\n   6433,\n   25578,\n   18912,\n   11253,\n   19337,\n   7852,\n   5770,\n   7944,\n   7790,\n   8125,\n   18595,\n   8351,\n   5567,\n   6923,\n   6141,\n   4773,\n   8553,\n   5927,\n   5145,\n   8395,\n   4943,\n   4681,\n   19600]),\n (12,\n  [9704,\n   8474,\n   30028,\n   14840,\n   16556,\n   12269,\n   25425,\n   9940,\n   25117,\n   9286,\n   14622,\n   11979,\n   12839,\n   21164,\n   26284,\n   22743,\n   10690,\n   13656,\n   9871,\n   17376,\n   4580,\n   13019,\n   9048,\n   9711,\n   18475,\n   11128,\n   9033,\n   15866,\n   9923,\n   14933,\n   9561])]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291739688,"submitTime":1500291729654,"finishTime":1500291751516,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d241d4df-db63-45d5-b7a3-2be38c818a9c"},{"version":"CommandV1","origId":1456266427964848,"guid":"01b04cb5-cd33-4701-9764-bb058dc63d10","subtype":"command","commandType":"auto","position":4.0,"command":"dataMeans=dataRDD.map(lambda x:(x[0],median(x[1])))\ndataQuantile=dataRDD.map(lambda x:(x[0],quantile(x[1])))\ndataQuantile=dataQuantile.map(lambda x:(int(x[0]),float(x[1])))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-29-775de38c0eaa&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">3</span>\n<span class=\"ansiyellow\">    dataQuantile=dataQuantile.map(lambda x:(int(x[0]),float(x[1]))))</span>\n<span class=\"ansigrey\">                                                                   ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n</div>","workflows":[],"startTime":1500291751560,"submitTime":1500291729669,"finishTime":1500291751599,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"30d54423-9961-4ea5-b74f-40e0766024a7"},{"version":"CommandV1","origId":1456266427964849,"guid":"c595f85f-00ca-4c8d-83b3-e2a134f87780","subtype":"command","commandType":"auto","position":3.5,"command":"import numpy as np\ndef median(values):\n  return np.median(np.array(values))\ndef quantile(values):\n  return np.percentile(np.array(values),75)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291751519,"submitTime":1500291729662,"finishTime":1500291751557,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2b82fecd-53de-404a-ae0b-2601d38e23d7"},{"version":"CommandV1","origId":1456266427964850,"guid":"c8ef7d22-ef34-4d93-ba88-c8acf95bdf96","subtype":"command","commandType":"auto","position":5.0,"command":"dataMeans.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>\n[(1, 6239.0),\n (2, 4473.0),\n (3, 5049.0),\n (4, 10399.0),\n (5, 6340.5),\n (7, 8791.0),\n (8, 9057.5),\n (9, 13406.0),\n (10, 10955.0),\n (11, 7821.0),\n (12, 12839.0)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291770240,"submitTime":1500291729698,"finishTime":1500291771549,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"54c06a2c-3736-4d77-bbc6-d0a14705cdc9"},{"version":"CommandV1","origId":1456266427964851,"guid":"805c2d19-4649-448c-b00a-f599691222b9","subtype":"command","commandType":"auto","position":6.0,"command":"january=[[x] for x in monthsRDDs[0]]\ndf=sqlContext.createDataFrame(data=january,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[2714],[2773],[3101],[3343],[3559],[3641],[3678],[3872],[4255],[4943],[5144],[5437],[5786],[5805],[6178],[6239],[6381],[6549],[6882],[7231],[7376],[7502],[10317],[10783],[11992],[12016],[12917],[14037],[14591],[15037],[17892]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":"<span class=\"ansired\">TypeError</span>: Can not infer schema for type: &lt;type &apos;int&apos;&gt;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-34-1eaacf36c3e3&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">from</span> pyspark<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">.</span>types <span class=\"ansigreen\">import</span> IntegerType<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">=</span>sqlContext<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">=</span>monthsRDDs<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>schema<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;days&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> df<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/context.pyc</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    331</span>         Py4JJavaError<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">.</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    332</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 333</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>sparkSession<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">,</span> verifySchema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    334</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    335</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.pyc</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    524</span>             rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    525</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 526</span><span class=\"ansiyellow\">             </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    527</span>         jrdd <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>SerDeUtil<span class=\"ansiyellow\">.</span>toJavaArray<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">.</span>_to_java_object_rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    528</span>         jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>applySchemaToPythonRDD<span class=\"ansiyellow\">(</span>jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.pyc</span> in <span class=\"ansicyan\">_createFromLocal</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    388</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    389</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>list<span class=\"ansiyellow\">,</span> tuple<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 390</span><span class=\"ansiyellow\">             </span>struct <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_inferSchemaFromList<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    391</span>             converter <span class=\"ansiyellow\">=</span> _create_converter<span class=\"ansiyellow\">(</span>struct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    392</span>             data <span class=\"ansiyellow\">=</span> map<span class=\"ansiyellow\">(</span>converter<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.pyc</span> in <span class=\"ansicyan\">_inferSchemaFromList</span><span class=\"ansiblue\">(self, data)</span>\n<span class=\"ansigreen\">    320</span>             warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n<span class=\"ansigreen\">    321</span>                           &quot;please use pyspark.sql.Row instead&quot;)\n<span class=\"ansigreen\">--&gt; 322</span><span class=\"ansiyellow\">         </span>schema <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>_merge_type<span class=\"ansiyellow\">,</span> map<span class=\"ansiyellow\">(</span>_infer_schema<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    323</span>         <span class=\"ansigreen\">if</span> _has_nulltype<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    324</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Some of types cannot be determined after inferring&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/types.pyc</span> in <span class=\"ansicyan\">_infer_schema</span><span class=\"ansiblue\">(row)</span>\n<span class=\"ansigreen\">    990</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    991</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 992</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">raise</span> TypeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Can not infer schema for type: %s&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    993</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    994</span>     fields <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>StructField<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">,</span> _infer_type<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> True<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> items<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: Can not infer schema for type: &lt;type &apos;int&apos;&gt;</div>","workflows":[],"startTime":1500291772916,"submitTime":1500291729723,"finishTime":1500291773937,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"0774dd79-b14b-4804-862c-7340fd92f556"},{"version":"CommandV1","origId":1456266427964853,"guid":"23a85537-dd95-498f-a0d3-293c320c752f","subtype":"command","commandType":"auto","position":8.0,"command":"february=[[x] for x in monthsRDDs[1]]\ndf=sqlContext.createDataFrame(data=february,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1774],[2568],[3180],[3202],[3267],[3277],[3310],[3489],[3727],[4160],[4201],[4251],[4289],[4426],[4520],[4613],[4916],[4973],[5045],[5064],[5129],[5707],[5716],[6875],[7447],[8529],[9036],[10766]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1500291773940,"submitTime":1500291729730,"finishTime":1500291774172,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4abb8502-3bea-45fc-a5dc-ad63215460f8"},{"version":"CommandV1","origId":1456266427964854,"guid":"e828a4d1-ccd2-49f2-a15b-4f871929e7ea","subtype":"command","commandType":"auto","position":9.0,"command":"march=[[x] for x in monthsRDDs[2]]\ndf=sqlContext.createDataFrame(data=march,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[3746],[4073],[4359],[4373],[4431],[4540],[4550],[4624],[4686],[4759],[4813],[4818],[4836],[4878],[5032],[5049],[5116],[5299],[5306],[5316],[5498],[5619],[5758],[5934],[6778],[7383],[8977],[9833],[9972],[10010],[10529]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1500291774176,"submitTime":1500291729736,"finishTime":1500291774455,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":["<id>"],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d4bd5997-90e3-487a-94a5-323336368ad5"},{"version":"CommandV1","origId":1456266427964855,"guid":"739548a3-619e-42ff-9395-de67743bfb79","subtype":"command","commandType":"auto","position":10.0,"command":"april=[[x] for x in monthsRDDs[3]]\ndf=sqlContext.createDataFrame(data=april,schema=['days'])\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[2527],[2550],[4964],[5190],[5525],[5678],[5837],[7073],[7417],[7475],[9346],[9886],[9998],[10076],[10722],[11031],[11401],[11667],[11905],[12653],[13362],[15903],[16062],[16601],[20158],[20331],[22741],[26668]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"days","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":1500291774459,"submitTime":1500291729742,"finishTime":1500291774740,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["days"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"10"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"a656c181-20d4-4a6c-9579-6f18700def34"},{"version":"CommandV1","origId":2751569960862873,"guid":"8d0d61df-69a0-430c-b1af-994ce91d329e","subtype":"command","commandType":"auto","position":4.5,"command":"dataQuantile.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>\n[(1, 10550.0),\n (2, 5273.5),\n (3, 5846.0),\n (4, 13997.25),\n (5, 6792.0),\n (7, 10448.0),\n (8, 9888.5),\n (9, 15345.0),\n (10, 13545.5),\n (11, 8916.5),\n (12, 16966.0)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 72.0 failed 1 times, most recent failure: Lost task 2.0 in stage 72.0 (TID 1137, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-28-abb9ecee7766&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>dataQuantile<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    806</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    807</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 808</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    809</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    810</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 72.0 failed 1 times, most recent failure: Lost task 2.0 in stage 72.0 (TID 1137, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 171, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;&lt;ipython-input-27-4bd45fbcffa6&gt;&quot;, line 3, in &lt;lambda&gt;\nTypeError: integer argument expected, got float\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1950)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1963)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1977)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 171, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;&lt;ipython-input-27-4bd45fbcffa6&gt;&quot;, line 3, in &lt;lambda&gt;\nTypeError: integer argument expected, got float\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n</div>","workflows":[],"startTime":1500291751602,"submitTime":1500291729675,"finishTime":1500291752811,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"ff96a4ed-babe-4afd-8abe-6ba65c1673ed"},{"version":"CommandV1","origId":2751569960862874,"guid":"feaa2f03-901e-48a0-bb67-1a009e68e5de","subtype":"command","commandType":"auto","position":4.75,"command":"%sql DROP TABLE quantiles","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{}},"errorSummary":"Error in SQL statement: NoSuchTableException: Table or view 'quantiles' not found in database 'default';","error":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'quantiles' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropTable(SessionCatalog.scala:540)\n\tat org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:208)\n\tat com.databricks.sql.acl.TrustedRunnableCommand$$anonfun$run$1.apply(TrustedRunnableCommand.scala:29)\n\tat com.databricks.sql.acl.TrustedRunnableCommand$$anonfun$run$1.apply(TrustedRunnableCommand.scala:29)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:460)\n\tat com.databricks.sql.acl.TrustedRunnableCommand.run(TrustedRunnableCommand.scala:29)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:599)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:698)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:82)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:28)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:28)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$2.apply(DriverLocal.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$2.apply(DriverLocal.scala:211)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:173)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:168)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:39)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:206)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:39)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:211)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:488)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:348)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:116)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$2.apply(DriverLocal.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$2.apply(DriverLocal.scala:211)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:173)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:168)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:39)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:206)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:39)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:211)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:488)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:348)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n\tat java.lang.Thread.run(Thread.java:745)\n","workflows":[],"startTime":1500291752814,"submitTime":1500291729683,"finishTime":1500291760354,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"dde6dc39-626c-4a54-85b9-7d06e2e6ea61"},{"version":"CommandV1","origId":2751569960862875,"guid":"d8891557-aab4-42cb-84cb-4bbc4730e364","subtype":"command","commandType":"auto","position":4.875,"command":"\nquantilesDF=sqlContext.createDataFrame(dataQuantile,schema=[\"month\",\"quantile\"])\nquantilesDF.write.saveAsTable('quantiles')","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 69.0 failed 1 times, most recent failure: Lost task 1.0 in stage 69.0 (TID 1132, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-26-8a6454313ead&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> dataQuantile<span class=\"ansiyellow\">=</span>dataQuantile<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>float<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>quantilesDF<span class=\"ansiyellow\">=</span>sqlContext<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>dataQuantile<span class=\"ansiyellow\">,</span>schema<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&quot;month&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;quantile&quot;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> quantilesDF<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>saveAsTable<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;quantiles&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    331</span>         Py4JJavaError<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">.</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    332</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 333</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>sparkSession<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">,</span> verifySchema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    334</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    335</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    522</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    523</span>         <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 524</span><span class=\"ansiyellow\">             </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    525</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    526</span>             rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_createFromRDD</span><span class=\"ansiblue\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansigreen\">    362</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    363</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>list<span class=\"ansiyellow\">,</span> tuple<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 364</span><span class=\"ansiyellow\">             </span>struct <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_inferSchema<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    365</span>             converter <span class=\"ansiyellow\">=</span> _create_converter<span class=\"ansiyellow\">(</span>struct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    366</span>             rdd <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>converter<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_inferSchema</span><span class=\"ansiblue\">(self, rdd, samplingRatio)</span>\n<span class=\"ansigreen\">    333</span>         <span class=\"ansiyellow\">:</span><span class=\"ansigreen\">return</span><span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">:</span><span class=\"ansigreen\">class</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">&#96;</span>pyspark<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">.</span>types<span class=\"ansiyellow\">.</span>StructType<span class=\"ansiyellow\">&#96;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    334</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 335</span><span class=\"ansiyellow\">         </span>first <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>first<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    336</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> first<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    337</span>             raise ValueError(&quot;The first row in RDD is empty, &quot;\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">first</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1358</span>         ValueError<span class=\"ansiyellow\">:</span> RDD <span class=\"ansigreen\">is</span> empty<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1359</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1360</span><span class=\"ansiyellow\">         </span>rs <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1361</span>         <span class=\"ansigreen\">if</span> rs<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1362</span>             <span class=\"ansigreen\">return</span> rs<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">take</span><span class=\"ansiblue\">(self, num)</span>\n<span class=\"ansigreen\">   1340</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1341</span>             p <span class=\"ansiyellow\">=</span> range<span class=\"ansiyellow\">(</span>partsScanned<span class=\"ansiyellow\">,</span> min<span class=\"ansiyellow\">(</span>partsScanned <span class=\"ansiyellow\">+</span> numPartsToTry<span class=\"ansiyellow\">,</span> totalParts<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1342</span><span class=\"ansiyellow\">             </span>res <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> takeUpToNumLeft<span class=\"ansiyellow\">,</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1343</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1344</span>             items <span class=\"ansiyellow\">+=</span> res<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">runJob</span><span class=\"ansiblue\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansigreen\">    966</span>         <span class=\"ansired\"># SparkContext#runJob.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    967</span>         mappedRDD <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>partitionFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 968</span><span class=\"ansiyellow\">         </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsc<span class=\"ansiyellow\">.</span>sc<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">,</span> partitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    969</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    970</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 69.0 failed 1 times, most recent failure: Lost task 1.0 in stage 69.0 (TID 1132, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 171, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-26-8a6454313ead&gt;&quot;, line 1, in &lt;lambda&gt;\nTypeError: integer argument expected, got float\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1950)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1963)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 171, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-26-8a6454313ead&gt;&quot;, line 1, in &lt;lambda&gt;\nTypeError: integer argument expected, got float\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n</div>","workflows":[],"startTime":1500291760357,"submitTime":1500291729690,"finishTime":1500291770233,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"amarildo.likmeta@mail.polimi.it","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"01ba804f-3c53-4dad-ab75-034ce4cc4c04"}],"dashboards":[],"guid":"fa26b366-1f9a-4ff8-be77-96b1ae18acaf","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}}